{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae2ea47-d7f4-4d1d-8e70-4b58c448433f",
   "metadata": {},
   "source": [
    "%%capture\n",
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af9b8113-c0a9-4bf9-afdf-70835da29e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minsearch\n",
    "from openai import OpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96cc3476-71c3-4525-a096-6476b81afd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../documents.json', 'rt') as f:\n",
    "    docs_raw = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a47413-d09d-4d52-b022-0c8e89bc310f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "948"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for course in docs_raw:\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course['course']\n",
    "        documents.append(doc)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a8f5f6b-42a9-44ec-aa67-ce6fddd1887e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cea686a-99a3-4b2a-a342-7207964dd2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ad72987-2ac4-4336-b640-f5738224896a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x79b34b9db8f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a783df6-7ef7-4793-831a-0f3d3234295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"How to understand evaluation metrics?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03ffdbe5-7e5d-422e-9003-daab3d2b9bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = {'question': 3.0, 'section':0.5}\n",
    "data = index.search(\n",
    "    query = q,\n",
    "    boost_dict=boost,\n",
    "    num_results=5,\n",
    "    filter_dict={'course':'machine-learning-zoomcamp'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b50b7aa-b624-4a8b-a19e-31aede9d693d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\\nUse classification_report from sklearn. For more info check here.\\nAbhishek N',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'How to get all classification metrics?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'text': \"It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\\nNonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\",\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'I didn’t fully understand the ROC curve. Can I move on?',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'text': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\\nDiego Giraldo',\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'What dataset should I use to compute the metrics in Question 3',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'text': \"Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\\nfrom sklearn.metrics import (accuracy_score,\\nprecision_score,\\nrecall_score,\\nf1_score,\\nroc_auc_score\\n)\\naccuracy = accuracy_score(y_val, y_pred)\\nprecision = precision_score(y_val, y_pred)\\nrecall = recall_score(y_val, y_pred)\\nf1 = f1_score(y_val, y_pred)\\nroc_auc = roc_auc_score(y_val, y_pred)\\nprint(f'Accuracy: {accuracy}')\\nprint(f'Precision: {precision}')\\nprint(f'Recall: {recall}')\\nprint(f'F1-Score: {f1}')\\nprint(f'ROC AUC: {roc_auc}')\\n(Harish Balasundaram)\",\n",
       "  'section': '4. Evaluation Metrics for Classification',\n",
       "  'question': 'Evaluate the Model using scikit learn metrics',\n",
       "  'course': 'machine-learning-zoomcamp'},\n",
       " {'text': 'During the XGBoost lesson, we created a parser to extract the training and validation auc from the standard output. However, we can accomplish that in a more straightforward way.\\nWe can use the evals_result  parameters, which takes an empty dictionary and updates it for each tree. Additionally, you can store the data in a dataframe and plot it in an easier manner.\\nAdded by Daniel Coronel',\n",
       "  'section': '6. Decision Trees and Ensemble Learning',\n",
       "  'question': 'How to get the training and validation metrics from XGBoost?',\n",
       "  'course': 'machine-learning-zoomcamp'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a999c6-8dca-485d-ae39-1e8f10bf2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teacher assistant. Answer the question based on the context.\n",
    "Use only the facts from the context. If relevant information is missing, say you do not know.\n",
    "Question:\n",
    "{question}\n",
    "Context:\n",
    "{context}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b795781-e00f-4a63-94ea-8d438add6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\n",
    "for doc in data:\n",
    "    context += f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer:{doc['text']}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1455817-7aab-4351-9603-6e3fb30baf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(question=q, context=context).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82526cba-cfd0-4dde-b202-3d4aef6db47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teacher assistant. Answer the question based on the context.\n",
      "Use only the facts from the context. If relevant information is missing, say you do not know.\n",
      "Question:\n",
      "How to understand evaluation metrics?\n",
      "Context:\n",
      "section: 4. Evaluation Metrics for Classification\n",
      "question: How to get all classification metrics?\n",
      "answer:How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\n",
      "Use classification_report from sklearn. For more info check here.\n",
      "Abhishek N\n",
      "\n",
      "section: 4. Evaluation Metrics for Classification\n",
      "question: I didn’t fully understand the ROC curve. Can I move on?\n",
      "answer:It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\n",
      "Nonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\n",
      "\n",
      "section: 4. Evaluation Metrics for Classification\n",
      "question: What dataset should I use to compute the metrics in Question 3\n",
      "answer:You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\n",
      "Diego Giraldo\n",
      "\n",
      "section: 4. Evaluation Metrics for Classification\n",
      "question: Evaluate the Model using scikit learn metrics\n",
      "answer:Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\n",
      "from sklearn.metrics import (accuracy_score,\n",
      "precision_score,\n",
      "recall_score,\n",
      "f1_score,\n",
      "roc_auc_score\n",
      ")\n",
      "accuracy = accuracy_score(y_val, y_pred)\n",
      "precision = precision_score(y_val, y_pred)\n",
      "recall = recall_score(y_val, y_pred)\n",
      "f1 = f1_score(y_val, y_pred)\n",
      "roc_auc = roc_auc_score(y_val, y_pred)\n",
      "print(f'Accuracy: {accuracy}')\n",
      "print(f'Precision: {precision}')\n",
      "print(f'Recall: {recall}')\n",
      "print(f'F1-Score: {f1}')\n",
      "print(f'ROC AUC: {roc_auc}')\n",
      "(Harish Balasundaram)\n",
      "\n",
      "section: 6. Decision Trees and Ensemble Learning\n",
      "question: How to get the training and validation metrics from XGBoost?\n",
      "answer:During the XGBoost lesson, we created a parser to extract the training and validation auc from the standard output. However, we can accomplish that in a more straightforward way.\n",
      "We can use the evals_result  parameters, which takes an empty dictionary and updates it for each tree. Additionally, you can store the data in a dataframe and plot it in an easier manner.\n",
      "Added by Daniel Coronel\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5e6137c-1c71-4352-8110-8afd01b0f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0289f802-f3ab-4e28-a1d3-c3a3f3fb01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': prompt }\n",
    "             ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bac0c65f-c117-4bdb-ada4-fc17d38ce0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To understand evaluation metrics for classification, you can compute several key metrics such as precision, recall, F1 score, accuracy, and ROC AUC using scikit-learn's built-in functions. This approach is more accurate and efficient than manual calculations. For example, you can use functions like `accuracy_score`, `precision_score`, `recall_score`, `f1_score`, and `roc_auc_score` on your validation dataset predictions.\\n\\nAdditionally, to get all classification metrics together (precision, recall, f1 score, accuracy) simultaneously, you can use the `classification_report` function from scikit-learn, which summarizes these metrics in one output.\\n\\nIt is important to use the validation dataset (e.g., `dt_val`) to compute these metrics after model training.\\n\\nIf you are dealing with complex metrics like the ROC curve, it might take time to fully understand, and it's okay to move on initially and revisit the topic later through different resources.\\n\\nIn summary:\\n- Use scikit-learn functions to calculate metrics.\\n- Use `classification_report` for a combined metric summary.\\n- Use your validation dataset for evaluation.\\n- Take time to understand complex metrics like ROC curve if needed.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7259200b-6156-4b1e-a095-5272608b6e17",
   "metadata": {},
   "source": [
    "To understand evaluation metrics for classification, you can compute several key metrics such as precision, recall, F1 score, accuracy, and ROC AUC using scikit-learn's built-in functions. This approach is more accurate and efficient than manual calculations. For example, you can use functions like `accuracy_score`, `precision_score`, `recall_score`, `f1_score`, and `roc_auc_score` on your validation dataset predictions.\\n\\nAdditionally, to get all classification metrics together (precision, recall, f1 score, accuracy) simultaneously, you can use the `classification_report` function from scikit-learn, which summarizes these metrics in one output.\\n\\nIt is important to use the validation dataset (e.g., `dt_val`) to compute these metrics after model training.\\n\\nIf you are dealing with complex metrics like the ROC curve, it might take time to fully understand, and it's okay to move on initially and revisit the topic later through different resources.\\n\\nIn summary:\\n- Use scikit-learn functions to calculate metrics.\\n- Use `classification_report` for a combined metric summary.\\n- Use your validation dataset for evaluation.\\n- Take time to understand complex metrics like ROC curve if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3d4d0-27f1-4f4e-b680-952ab8c33623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
